{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6AnMrbA9K2yhVIFEpbvc+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XaviPrograms/Big_Data_ML_AI/blob/main/Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekifyNc5NZCE",
        "outputId": "e6d7d0c9-e8da-4f0c-cd05-21d4e9a7a877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, spark i j k) --> prob=[0.6292098489668483,0.3707901510331517], prediction=0.000000\n",
            "(5, l m n) --> prob=[0.984770006762304,0.015229993237696027], prediction=0.000000\n",
            "(6, spark hadoop spark) --> prob=[0.13412348342566147,0.8658765165743385], prediction=1.000000\n",
            "(7, apache hadoop) --> prob=[0.9955732114398529,0.00442678856014711], prediction=0.000000\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "\n",
        "# Instead of creating a new SparkContext, get the existing one or create if none exists.\n",
        "try:\n",
        "    sc = SparkContext.getOrCreate()\n",
        "except ValueError:\n",
        "    sc = SparkContext('local')\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# Prepare training documents from a list of (id, text, label) tuples.\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "model = pipeline.fit(training)\n",
        "\n",
        "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"spark hadoop spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "# Make predictions on test documents and print columns of interest.\n",
        "prediction = model.transform(test)\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    rid, text, prob, prediction = row\n",
        "    print(\n",
        "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
        "            rid, text, str(prob), prediction  # type: ignore\n",
        "        )\n",
        "    )"
      ]
    }
  ]
}